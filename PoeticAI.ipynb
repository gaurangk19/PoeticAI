{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Oi99E5LjOctJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import Activation, Dense, LSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = tf.keras.utils.get_file('shakespeare.txt',\n",
        "        'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgDhwRduPDnd",
        "outputId": "9186ba24-6f10-4370-ade8-7d9e19a17927"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[300000:800000]"
      ],
      "metadata": {
        "id": "XYW0URiWQRCx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(set(text))\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))"
      ],
      "metadata": {
        "id": "Exf-f72aQcnv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3\n",
        "\n",
        "sentences = []\n",
        "next_char = []"
      ],
      "metadata": {
        "id": "6rSQpO2lUg0H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "    sentences.append(text[i: i + SEQ_LENGTH])\n",
        "    next_char.append(text[i + SEQ_LENGTH])"
      ],
      "metadata": {
        "id": "v2vlCsunUiWi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((len(sentences), SEQ_LENGTH,\n",
        "              len(characters)), dtype=bool)\n",
        "y = np.zeros((len(sentences),\n",
        "              len(characters)), dtype=bool)\n",
        "\n",
        "for i, satz in enumerate(sentences):\n",
        "    for t, char in enumerate(satz):\n",
        "        x[i, t, char_to_index[char]] = 1\n",
        "    y[i, char_to_index[next_char[i]]] = 1"
      ],
      "metadata": {
        "id": "9bimQNO-Uj7Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128,\n",
        "               input_shape=(SEQ_LENGTH,\n",
        "                            len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_u6Hkl3Ulzr",
        "outputId": "5d43fb1a-8527-432f-99ae-646a93f8eefb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.01))\n",
        "\n",
        "model.fit(x, y, batch_size=256, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olzgsSPhU6va",
        "outputId": "679d8533-c628-49b4-d058-97299a32a049"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2.4819\n",
            "Epoch 2/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1.7889\n",
            "Epoch 3/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 1.6113\n",
            "Epoch 4/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1.5306\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e68f965b040>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "eGHRBDofU8OB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_predictions[0, t, char_to_index[char]] = 1\n",
        "\n",
        "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
        "        next_index = sample(predictions,\n",
        "                                 temperature)\n",
        "        next_character = index_to_char[next_index]\n",
        "\n",
        "        generated += next_character\n",
        "        sentence = sentence[1:] + next_character\n",
        "    return generated"
      ],
      "metadata": {
        "id": "VdyymrpnVHcy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(300, 0.2))\n",
        "print(generate_text(300, 0.4))\n",
        "print(generate_text(300, 0.5))\n",
        "print(generate_text(300, 0.6))\n",
        "print(generate_text(300, 0.7))\n",
        "print(generate_text(300, 0.8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3DIYqyOVKZI",
        "outputId": "ff60c681-3183-46f8-96e9-4045458247a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ed withal.\n",
            "\n",
            "nurse:\n",
            "anon, anon!\n",
            "come, let the senter the sention to the heads.\n",
            "\n",
            "henry bolingbroke:\n",
            "my lord of his sunt the senter with her hate.\n",
            "\n",
            "henry bolingbroke:\n",
            "i am not the senter hath the sention.\n",
            "\n",
            "paris:\n",
            "i see the senter of his son,\n",
            "and the senting the sention the sentle the sence\n",
            "and the senter the senter and the senter.\n",
            "\n",
            "king henr\n",
            " sir, of such a man, who hath a\n",
            "daughter thou shall not my father's carrow\n",
            "should in the compose, i am the sends is the secking.\n",
            "\n",
            "henry bolingbroke:\n",
            "i am death is confult himself,\n",
            "and the best not it shall the best hate,\n",
            "and the best tell the wind the mounting the tone.\n",
            "\n",
            "romeo:\n",
            "and below what beceits for the mind and thee shall shall no b\n",
            "k:\n",
            "sweet rest his soul! fly, lords, and the founts.\n",
            "\n",
            "henry bolingbroke:\n",
            "you did now thee to the took and her\n",
            "his heart is the heart, in a sears the arrer!\n",
            "\n",
            "romeo:\n",
            "come on my son the fill, and she conceing thee love?\n",
            "\n",
            "polixenes:\n",
            "i am too fount and the man and will it,\n",
            "and grave their love be with my bands some\n",
            "past the world the hands the \n",
            "\n",
            "every good hap to you that chances here?\n",
            "\n",
            "fartilaun:\n",
            "hath servet the nursely space of lies,\n",
            "my lords a sain your sad with the sent face.\n",
            "\n",
            "paurin:\n",
            "our part: therefore be tengs, and book'd helld!\n",
            "\n",
            "juliet:\n",
            "the eward to the best, warwick and see inselp\n",
            "the ispition of all the faither is hate.\n",
            "\n",
            "prizce:\n",
            "whick with a coartest and thee strength\n",
            "\n",
            "s\n",
            "any in italy, and as soon moved to be teat\n",
            "she prison of my lords and unto my concornest\n",
            "in postices as pounts the prepent.\n",
            "\n",
            "queen margaret:\n",
            "gave sir; and selson bengled quithly say,\n",
            "and the stees in both all proudlerion\n",
            "the laids unlingly as boy, is see,\n",
            "i camillo, what is, their coutherow,\n",
            "what i have bead, an come fon wherefore count\n",
            " there? his train? camillo with him?\n",
            "\n",
            "first ie:\n",
            "how farewell, bestrain some at, in death.\n",
            "\n",
            "unnon:\n",
            "thou hast sweet the tises teer hand our saint.\n",
            "\n",
            "henry bolingbroke:\n",
            "my sor the morness well, the eight our sorrow.\n",
            "\n",
            "mercutio:\n",
            "why warwick to twill of for own to the death;\n",
            "and heart despite it, conlight the grace;\n",
            "the pounts that my some faint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1CMX9j-LVMEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}